{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install evaluate\n",
    "# %pip install datasets\n",
    "# %pip install accelerate -U\n",
    "# %pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rkre7\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# train a machine learning model to predict the genre of a song based on its lyrics with pytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a device depending on whats available\n",
    "* ´cuda´ for GPU\n",
    "* ´cpu´ for CPU\n",
    "* ´mps´ for Apple silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set random seeds to make sure results are reproducible\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# set device to cuda or mps if available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'sentence'],\n",
       "        num_rows: 155966\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'sentence'],\n",
       "        num_rows: 19496\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'sentence'],\n",
       "        num_rows: 19496\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "raw_datasets = load_dataset(\"csv\", data_files=\"data.csv\", split=\"train\")\n",
    "\n",
    "# change column name lyrics to sentence and playlist_genre to label\n",
    "raw_datasets = raw_datasets.rename_column(\"lyrics\", \"sentence\")\n",
    "raw_datasets = raw_datasets.rename_column(\"playlist_genre\", \"label\")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_testvalid = raw_datasets.train_test_split(test_size=0.2, seed=SEED)\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "# Assign the resulting datasets to variables\n",
    "train_dataset = train_testvalid[\"train\"]\n",
    "valid_dataset = test_valid[\"train\"]\n",
    "test_dataset = test_valid[\"test\"]\n",
    "\n",
    "# Now you have train_dataset, valid_dataset, and test_dataset\n",
    "raw_datasets = {\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": valid_dataset,\n",
    "    \"test\": test_dataset,\n",
    "}\n",
    "raw_datasets = datasets.DatasetDict(raw_datasets)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'sentence', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 155966\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'sentence', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 19496\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'sentence', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 19496\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Function to tokenize a single row\n",
    "def tokenize_function(row):\n",
    "    try:\n",
    "        # Ensure that 'lyrics' is always a string. Replace non-strings with a placeholder.\n",
    "        lyrics = [\n",
    "            str(lyric) if isinstance(lyric, str) else \"\" for lyric in row[\"sentence\"]\n",
    "        ]\n",
    "        return tokenizer(lyrics, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing row: {row}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Apply tokenization to each subset\n",
    "tokenized_train = raw_datasets[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_validation = raw_datasets[\"validation\"].map(tokenize_function, batched=True)\n",
    "tokenized_test = raw_datasets[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Combine back into a DatasetDict\n",
    "tokenized_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": tokenized_train,\n",
    "        \"validation\": tokenized_validation,\n",
    "        \"test\": tokenized_test,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check the result\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkre7\\AppData\\Local\\Temp\\ipykernel_17468\\4026832121.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy = load_metric(\"accuracy\")\n",
      "c:\\Users\\rkre7\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rkre7\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rkre7\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rkre7\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the metric function\n",
    "accuracy = load_metric(\"accuracy\")\n",
    "precision = load_metric(\"precision\")\n",
    "recall = load_metric(\"recall\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # Calculate precision, recall, and f1\n",
    "    prec, rec, f1_score, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/116976 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 78%|███████▊  | 91500/116976 [01:50<13:13, 32.11it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.084, 'learning_rate': 4.355765285186706e-06, 'epoch': 9.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 92000/116976 [03:38<1:29:20,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0879, 'learning_rate': 4.270277663794283e-06, 'epoch': 9.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 92500/116976 [05:26<1:27:22,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0871, 'learning_rate': 4.184790042401861e-06, 'epoch': 9.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 93000/116976 [07:14<1:25:32,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0824, 'learning_rate': 4.0993024210094386e-06, 'epoch': 9.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 93500/116976 [09:02<1:24:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0902, 'learning_rate': 4.013814799617016e-06, 'epoch': 9.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 94000/116976 [10:50<1:22:02,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0785, 'learning_rate': 3.928327178224594e-06, 'epoch': 9.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 94500/116976 [12:38<1:20:19,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0842, 'learning_rate': 3.842839556832171e-06, 'epoch': 9.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 95000/116976 [14:26<1:18:32,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0844, 'learning_rate': 3.7573519354397486e-06, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 95500/116976 [16:14<1:16:43,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0913, 'learning_rate': 3.6718643140473266e-06, 'epoch': 9.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 96000/116976 [18:02<1:15:05,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0919, 'learning_rate': 3.5863766926549037e-06, 'epoch': 9.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 96500/116976 [19:50<1:13:08,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0796, 'learning_rate': 3.5008890712624816e-06, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 97000/116976 [21:39<1:11:55,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0914, 'learning_rate': 3.4154014498700587e-06, 'epoch': 9.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 83%|████████▎ | 97480/116976 [24:55<1:07:23,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7385746240615845, 'eval_accuracy': 0.7715941731637259, 'eval_precision': 0.7721286606428058, 'eval_recall': 0.7715941731637259, 'eval_f1': 0.7716238520543374, 'eval_runtime': 91.5375, 'eval_samples_per_second': 212.984, 'eval_steps_per_second': 26.623, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 97500/116976 [24:59<1:20:18,  4.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0785, 'learning_rate': 3.3299138284776366e-06, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 98000/116976 [26:47<1:08:14,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0682, 'learning_rate': 3.2444262070852146e-06, 'epoch': 10.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 98500/116976 [28:36<1:06:15,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0556, 'learning_rate': 3.1589385856927917e-06, 'epoch': 10.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 99000/116976 [30:25<1:04:13,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0635, 'learning_rate': 3.0734509643003696e-06, 'epoch': 10.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 99500/116976 [32:16<1:04:38,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0673, 'learning_rate': 2.9879633429079476e-06, 'epoch': 10.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 100000/116976 [34:08<1:02:38,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0761, 'learning_rate': 2.9024757215155247e-06, 'epoch': 10.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 100500/116976 [35:58<56:37,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0689, 'learning_rate': 2.8169881001231026e-06, 'epoch': 10.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 101000/116976 [37:43<55:00,  4.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0593, 'learning_rate': 2.7315004787306797e-06, 'epoch': 10.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 101500/116976 [39:27<53:51,  4.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0671, 'learning_rate': 2.6460128573382576e-06, 'epoch': 10.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 102000/116976 [41:12<52:15,  4.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0772, 'learning_rate': 2.5605252359458356e-06, 'epoch': 10.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 102500/116976 [42:56<49:43,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0607, 'learning_rate': 2.4750376145534127e-06, 'epoch': 10.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 103000/116976 [44:40<48:11,  4.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0695, 'learning_rate': 2.3895499931609906e-06, 'epoch': 10.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 103500/116976 [46:24<46:22,  4.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0606, 'learning_rate': 2.304062371768568e-06, 'epoch': 10.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 104000/116976 [48:08<44:44,  4.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.063, 'learning_rate': 2.2185747503761456e-06, 'epoch': 10.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 104500/116976 [49:52<42:52,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.062, 'learning_rate': 2.133087128983723e-06, 'epoch': 10.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 105000/116976 [51:36<41:09,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0721, 'learning_rate': 2.047599507591301e-06, 'epoch': 10.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 105500/116976 [53:20<39:27,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0622, 'learning_rate': 1.9621118861988786e-06, 'epoch': 10.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 106000/116976 [55:04<37:51,  4.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0733, 'learning_rate': 1.8766242648064561e-06, 'epoch': 10.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 106500/116976 [56:48<36:03,  4.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0658, 'learning_rate': 1.7911366434140337e-06, 'epoch': 10.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 107000/116976 [58:32<34:15,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.077, 'learning_rate': 1.7056490220216116e-06, 'epoch': 10.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 92%|█████████▏| 107228/116976 [1:00:48<32:09,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.79966139793396, 'eval_accuracy': 0.7745691423881822, 'eval_precision': 0.7745954563806603, 'eval_recall': 0.7745691423881822, 'eval_f1': 0.7745381877578411, 'eval_runtime': 88.2394, 'eval_samples_per_second': 220.944, 'eval_steps_per_second': 27.618, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 107500/116976 [1:01:46<33:55,  4.65it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0599, 'learning_rate': 1.6201614006291891e-06, 'epoch': 11.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 108000/116976 [1:03:35<32:13,  4.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0497, 'learning_rate': 1.5346737792367666e-06, 'epoch': 11.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 108500/116976 [1:05:23<30:32,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.051, 'learning_rate': 1.4491861578443442e-06, 'epoch': 11.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 109000/116976 [1:07:12<28:34,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.059, 'learning_rate': 1.3636985364519217e-06, 'epoch': 11.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 109500/116976 [1:09:00<26:45,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0514, 'learning_rate': 1.2782109150594996e-06, 'epoch': 11.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 110000/116976 [1:10:49<24:49,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0611, 'learning_rate': 1.1927232936670771e-06, 'epoch': 11.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 110500/116976 [1:12:37<23:07,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0503, 'learning_rate': 1.1072356722746546e-06, 'epoch': 11.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 111000/116976 [1:14:25<22:05,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0487, 'learning_rate': 1.0217480508822324e-06, 'epoch': 11.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 111500/116976 [1:16:16<20:10,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0636, 'learning_rate': 9.362604294898099e-07, 'epoch': 11.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 112000/116976 [1:18:07<18:19,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0568, 'learning_rate': 8.507728080973876e-07, 'epoch': 11.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 112500/116976 [1:19:57<16:26,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0621, 'learning_rate': 7.652851867049651e-07, 'epoch': 11.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 113000/116976 [1:21:47<14:22,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0475, 'learning_rate': 6.797975653125429e-07, 'epoch': 11.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 113500/116976 [1:23:37<13:06,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0564, 'learning_rate': 5.943099439201204e-07, 'epoch': 11.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 114000/116976 [1:25:29<11:15,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0491, 'learning_rate': 5.08822322527698e-07, 'epoch': 11.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 114500/116976 [1:27:21<09:05,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0579, 'learning_rate': 4.2333470113527563e-07, 'epoch': 11.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 115000/116976 [1:29:10<06:54,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0577, 'learning_rate': 3.3784707974285326e-07, 'epoch': 11.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 115500/116976 [1:30:56<05:14,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0634, 'learning_rate': 2.523594583504309e-07, 'epoch': 11.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 116000/116976 [1:32:42<03:21,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0505, 'learning_rate': 1.668718369580085e-07, 'epoch': 11.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 116500/116976 [1:34:28<01:39,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0536, 'learning_rate': 8.13842155655861e-08, 'epoch': 11.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      "100%|██████████| 116976/116976 [1:37:41<00:00, 19.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8341795206069946, 'eval_accuracy': 0.7761592121460813, 'eval_precision': 0.7763562434187938, 'eval_recall': 0.7761592121460813, 'eval_f1': 0.7761422177522612, 'eval_runtime': 92.0434, 'eval_samples_per_second': 211.813, 'eval_steps_per_second': 26.477, 'epoch': 12.0}\n",
      "{'train_runtime': 5861.1948, 'train_samples_per_second': 319.319, 'train_steps_per_second': 19.958, 'train_loss': 0.01489559734361701, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=116976, training_loss=0.01489559734361701, metrics={'train_runtime': 5861.1948, 'train_samples_per_second': 319.319, 'train_steps_per_second': 19.958, 'train_loss': 0.01489559734361701, 'epoch': 12.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train from latest checkpoint\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# trainer.save_model(\"genre_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./genre_model\")\n",
    "\n",
    "# trainer from checkpoint\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [01:29<00:00, 27.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.8341795206069946,\n",
       " 'eval_accuracy': 0.7761592121460813,\n",
       " 'eval_precision': 0.7763562434187938,\n",
       " 'eval_recall': 0.7761592121460813,\n",
       " 'eval_f1': 0.7761422177522612,\n",
       " 'eval_runtime': 90.2115,\n",
       " 'eval_samples_per_second': 216.114,\n",
       " 'eval_steps_per_second': 27.014}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [01:32<00:00, 26.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.8546456098556519,\n",
       " 'eval_accuracy': 0.7769798933114485,\n",
       " 'eval_precision': 0.7770985621765307,\n",
       " 'eval_recall': 0.7769798933114485,\n",
       " 'eval_f1': 0.7769661126113228,\n",
       " 'eval_runtime': 92.5364,\n",
       " 'eval_samples_per_second': 210.685,\n",
       " 'eval_steps_per_second': 26.336}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19496/19496 [12:33<00:00, 25.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03179062902927399,\n",
       " 'eval_accuracy': 0.9851570214021005,\n",
       " 'eval_precision': 0.98521158918168,\n",
       " 'eval_recall': 0.9851570214021005,\n",
       " 'eval_f1': 0.9851423452199055,\n",
       " 'eval_runtime': 753.2059,\n",
       " 'eval_samples_per_second': 207.07,\n",
       " 'eval_steps_per_second': 25.884}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eminem - Rap God | RAP\n",
      " rap\n",
      "Tu tienes un gato muy bonito\n",
      " latin\n",
      "Paul Damixie x SERE - You Got Me Like | POP\n",
      " edm\n",
      "watashi wa raiku desu\n",
      " rock\n",
      "私はライクです\n",
      " rock\n"
     ]
    }
   ],
   "source": [
    "def predict_genre(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    # print(outputs.logits)\n",
    "    genre = outputs.logits.argmax(-1)\n",
    "    if genre == 0:\n",
    "        return \"pop\"\n",
    "    elif genre == 1:\n",
    "        return \"rap\"\n",
    "    elif genre == 2:\n",
    "        return \"rock\"\n",
    "    elif genre == 3:\n",
    "        return \"r&b\"\n",
    "    elif genre == 4:\n",
    "        return \"latin\"\n",
    "    elif genre == 5:\n",
    "        return \"edm\"\n",
    "    return 0\n",
    "\n",
    "# Eminem - Rap God | RAP\n",
    "print(\"Eminem - Rap God | RAP\\n\", predict_genre(\"Look, I was gonna go easy on you not to hurt your feelings.\")) # is in training data\n",
    "\n",
    "# random spanish sentence\n",
    "print(\"Tu tienes un gato muy bonito\\n\", predict_genre(\"Tu tienes un gato muy bonito\"))\n",
    "\n",
    "# Paul Damixie x SERE - You Got Me Like | POP\n",
    "print(\"Paul Damixie x SERE - You Got Me Like | POP\\n\", predict_genre(\"I tell myself that I'll be better off without you, but you and I know that's a lie. And I can't get you out of my mind. It's like you've got me hypnotized.\"))\n",
    "# is not in training data\n",
    "\n",
    "# compare Japanese with Latin letters and Japanese characters\n",
    "print(\"watashi wa raiku desu\\n\", predict_genre(\"watashi wa raiku desu\"))\n",
    "print(\"私はライクです\\n\", predict_genre(\"私はライクです\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.2529,  9.0998, -3.0546, -6.2239, -0.2718, -5.0436]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Eminem - Rap God | RAP\n",
      " rap\n",
      "tensor([[-3.6563, -1.4989, -1.4896, -2.7540,  2.9963, -4.0533]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Tu tienes un gato muy bonito\n",
      " latin\n",
      "tensor([[-2.5010, -6.6259, -5.5518, -5.0577, -5.0102, 11.3497]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Paul Damixie x SERE - You Got Me Like | POP\n",
      " edm\n",
      "tensor([[-0.4940, -1.6997,  5.9809, -7.5397, -1.2829, -4.5777]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "watashi wa raiku desu\n",
      " rock\n",
      "tensor([[-3.7927, -2.7125, 10.7992, -6.3658, -2.3098, -5.7035]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "私はライクです\n",
      " rock\n"
     ]
    }
   ],
   "source": [
    "def predict_genre(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs.logits)\n",
    "    genre = outputs.logits.argmax(-1)\n",
    "    if genre == 0:\n",
    "        return \"pop\"\n",
    "    elif genre == 1:\n",
    "        return \"rap\"\n",
    "    elif genre == 2:\n",
    "        return \"rock\"\n",
    "    elif genre == 3:\n",
    "        return \"r&b\"\n",
    "    elif genre == 4:\n",
    "        return \"latin\"\n",
    "    elif genre == 5:\n",
    "        return \"edm\"\n",
    "    return 0\n",
    "\n",
    "# Eminem - Rap God | RAP\n",
    "print(\"Eminem - Rap God | RAP\\n\", predict_genre(\"Look, I was gonna go easy on you not to hurt your feelings.\")) # is in training data\n",
    "\n",
    "# random spanish sentence\n",
    "print(\"Tu tienes un gato muy bonito\\n\", predict_genre(\"Tu tienes un gato muy bonito\"))\n",
    "\n",
    "# Paul Damixie x SERE - You Got Me Like | POP\n",
    "print(\"Paul Damixie x SERE - You Got Me Like | POP\\n\", predict_genre(\"I tell myself that I'll be better off without you, but you and I know that's a lie. And I can't get you out of my mind. It's like you've got me hypnotized.\"))\n",
    "# is not in training data\n",
    "\n",
    "# compare Japanese with Latin letters and Japanese characters\n",
    "print(\"watashi wa raiku desu\\n\", predict_genre(\"watashi wa raiku desu\"))\n",
    "print(\"私はライクです\\n\", predict_genre(\"私はライクです\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
